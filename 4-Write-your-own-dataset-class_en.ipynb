{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to write a custom dataset class\n",
    "\n",
    "A typical strategy to improve generalization in deep neural networks is to increase the number of training examples which allows more parameters to be used in the model. Even if the number of parameters is kept unchanged, increasing the number of training examples often improves generalization performance.\n",
    "\n",
    "Since it can be tedious and expensive to manually obtain and label additional training examples, a useful strategy is to consider methods for automatically increasing the size of a training set. Fortunately, for image datasets, there are several augmentation methods that have been found to work well in practice. They include:\n",
    "\n",
    "* Randomly cropping several slightly smaller images from the original training image.\n",
    "* Horizontally flipping the image.\n",
    "* Randomly rotating the image.\n",
    "* Applying various distortions and or noise to the images, etc.\n",
    "\n",
    "In this example, we will write a custom dataset class that performs the first two of these augmentation methods on the CIFAR10 dataset. We will then train our previous deep CNN and check that the generalization performance on the test set has in fact improved.\n",
    "\n",
    "We will create this dataset augmentation class as a subclass of `DatasetMixin`, which has the following API:\n",
    "\n",
    "- `__len__` method to return the size of data in dataset. \n",
    "- `get_example` method to return data or a tuple of data and label, which are passed by `i` argument variable. \n",
    "\n",
    "Other necessary features for a dataset can be prepared by inheriting `chainer.dataset.DasetMixin` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write the dataset augmentation class for CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer import dataset\n",
    "from chainer.datasets import cifar\n",
    "\n",
    "gpu_id = 0  # Set to -1 if you don't have a GPU\n",
    "\n",
    "class CIFAR10Augmented(dataset.DatasetMixin):\n",
    "\n",
    "    def __init__(self, train=True):\n",
    "        train_data, test_data = cifar.get_cifar10()\n",
    "        if train:\n",
    "            self.data = train_data\n",
    "        else:\n",
    "            self.data = test_data\n",
    "        self.train = train\n",
    "        self.random_crop = 4\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        x, t = self.data[i]\n",
    "        if self.train:\n",
    "            x = x.transpose(1, 2, 0)\n",
    "            h, w, _ = x.shape\n",
    "            x_offset = np.random.randint(self.random_crop)\n",
    "            y_offset = np.random.randint(self.random_crop)\n",
    "            x = x[y_offset:y_offset + h - self.random_crop,\n",
    "                  x_offset:x_offset + w - self.random_crop]\n",
    "            if np.random.rand() > 0.5:\n",
    "                x = np.fliplr(x)\n",
    "            x = x.transpose(2, 0, 1)\n",
    "        return x, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class performs the following types of data augmentation on the CIFAR10 example images:\n",
    "\n",
    "- Randomly crop a 28X28 area form the 32X32 whole image data.\n",
    "- Randomly perform a horizontal flip with 0.5 probability. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train on the CIFAR10 dataset using our dataset augmentation class\n",
    "\n",
    "Let's now train the same deep CNN from the previous example. The only difference is that we will now use our dataset augmentation class. Since we reuse the same model with the same number of parameters, we can observe how much the augmentation improves the test set generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.90914     0.303409       1.62992               0.39371                   132.112       \n",
      "\u001b[J2           1.43191     0.478293       1.27838               0.54588                   264.048       \n",
      "\u001b[J3           1.18702     0.585227       1.09276               0.619626                  400.067       \n",
      "\u001b[J4           1.01345     0.653689       0.987466              0.677946                  539.112       \n",
      "\u001b[J5           0.873652    0.706322       0.845788              0.717954                  679.127       \n",
      "\u001b[J6           0.767949    0.747759       0.730195              0.756071                  818.885       \n",
      "\u001b[J7           0.687763    0.771767       0.694054              0.773985                  959.095       \n",
      "\u001b[J8           0.622564    0.794454       0.645117              0.787122                  1099.14       \n",
      "\u001b[J9           0.575068    0.811681       0.592126              0.805334                  1239.37       \n",
      "\u001b[J10          0.544989    0.820743       0.592981              0.809713                  1379.5        \n",
      "\u001b[J11          0.505111    0.832486       0.57005               0.820362                  1519.9        \n",
      "\u001b[J12          0.477515    0.84249        0.523909              0.829419                  1660.04       \n",
      "\u001b[J13          0.445599    0.85388        0.513789              0.838475                  1800.4        \n",
      "\u001b[J14          0.426495    0.860695       0.522605              0.838873                  1940.67       \n",
      "\u001b[J15          0.407493    0.866477       0.471392              0.850119                  2081.05       \n",
      "\u001b[J16          0.386167    0.871719       0.472904              0.846736                  2221.2        \n",
      "\u001b[J17          0.371217    0.878077       0.464127              0.852707                  2361.55       \n",
      "\u001b[J18          0.347782    0.884823       0.465015              0.859275                  2501.71       \n",
      "\u001b[J19          0.338677    0.887804       0.507209              0.852807                  2641.86       \n",
      "\u001b[J20          0.329545    0.891085       0.460554              0.860072                  2781.99       \n",
      "\u001b[J21          0.317989    0.894641       0.471952              0.86246                   2922.32       \n",
      "\u001b[J22          0.301315    0.899768       0.430734              0.868631                  3062.47       \n",
      "\u001b[J23          0.295803    0.903209       0.46246               0.865844                  3202.62       \n",
      "\u001b[J24          0.283452    0.90739        0.469402              0.866242                  3342.71       \n",
      "\u001b[J25          0.274753    0.909287       0.437473              0.871616                  3483.07       \n",
      "\u001b[J26          0.260213    0.916113       0.445745              0.867834                  3623.17       \n",
      "\u001b[J27          0.257125    0.913692       0.435107              0.87301                   3763.35       \n",
      "\u001b[J28          0.248685    0.919374       0.43123               0.872611                  3903.64       \n",
      "\u001b[J29          0.240966    0.920816       0.460309              0.8749                    4044.06       \n",
      "\u001b[J30          0.235967    0.922915       0.41632               0.875398                  4184.3        \n",
      "\u001b[J31          0.226795    0.925956       0.464462              0.876592                  4324.51       \n",
      "\u001b[J32          0.213103    0.929918       0.53401               0.868332                  4464.67       \n",
      "\u001b[J33          0.217985    0.929767       0.432384              0.880573                  4604.91       \n",
      "\u001b[J34          0.21266     0.930058       0.517659              0.875995                  4745.05       \n",
      "\u001b[J35          0.205731    0.933319       0.462973              0.879279                  4885.19       \n",
      "\u001b[J36          0.194878    0.93708        0.478156              0.878185                  5025.29       \n",
      "\u001b[J37          0.189911    0.9371         0.524318              0.877289                  5166.14       \n",
      "\u001b[J38          0.187328    0.939241       0.49479               0.881768                  5306.34       \n",
      "\u001b[J39          0.186668    0.9387         0.572656              0.880275                  5446.57       \n",
      "\u001b[J40          0.179926    0.941741       0.502882              0.873905                  5586.77       \n",
      "\u001b[J41          0.177103    0.941276       0.493753              0.882564                  5727.19       \n",
      "\u001b[J42          0.172091    0.943682       0.50923               0.883459                  5867.45       \n",
      "\u001b[J43          0.167118    0.945503       0.491199              0.883459                  6007.7        \n",
      "\u001b[J44          0.168332    0.944762       0.532451              0.885052                  6147.84       \n",
      "\u001b[J45          0.161096    0.94779        0.44879               0.88535                   6288.3        \n",
      "\u001b[J46          0.156804    0.947723       0.534743              0.880971                  6428.55       \n",
      "\u001b[J47          0.157067    0.948764       0.499726              0.883061                  6568.73       \n",
      "\u001b[J48          0.151593    0.950664       0.505575              0.884554                  6709.02       \n",
      "\u001b[J49          0.15124     0.950607       0.485807              0.888435                  6849.39       \n",
      "\u001b[J50          0.144955    0.953765       0.496659              0.884057                  6989.91       \n",
      "\u001b[J51          0.142699    0.953905       0.518727              0.881369                  7130.54       \n",
      "\u001b[J52          0.143991    0.953605       0.557215              0.890028                  7271.09       \n",
      "\u001b[J53          0.139029    0.955603       0.547734              0.884455                  7411.85       \n",
      "\u001b[J54          0.132937    0.957546       0.524091              0.889928                  7552.54       \n",
      "\u001b[J55          0.134308    0.955866       0.56463               0.880971                  7693.24       \n",
      "\u001b[J56          0.13513     0.956526       0.561894              0.886146                  7833.88       \n",
      "\u001b[J57          0.126692    0.959219       0.605839              0.882464                  7974.67       \n",
      "\u001b[J58          0.125418    0.959247       0.576991              0.891222                  8115.35       \n",
      "\u001b[J59          0.126186    0.959527       0.530535              0.884256                  8256.04       \n",
      "\u001b[J60          0.128873    0.957987       0.522743              0.891322                  8396.64       \n",
      "\u001b[J61          0.121862    0.960898       0.590277              0.886843                  8537.45       \n",
      "\u001b[J62          0.117387    0.962068       0.526727              0.888933                  8678.06       \n",
      "\u001b[J63          0.121539    0.961428       0.509131              0.887938                  8818.7        \n",
      "\u001b[J64          0.120479    0.961128       0.540537              0.890127                  8959.3        \n",
      "\u001b[J65          0.116914    0.961937       0.546361              0.883161                  9100.1        \n",
      "\u001b[J66          0.116331    0.963228       0.606167              0.886445                  9240.84       \n",
      "\u001b[J67          0.107006    0.965549       0.576042              0.892018                  9381.54       \n",
      "\u001b[J68          0.10937     0.964729       0.530554              0.892516                  9522.31       \n",
      "\u001b[J69          0.1097      0.964374       0.621504              0.890426                  9663.25       \n",
      "\u001b[J70          0.100521    0.96743        0.599571              0.883459                  9803.93       \n",
      "\u001b[J71          0.159817    0.953225       0.648678              0.873109                  9944.64       \n",
      "\u001b[J72          0.134413    0.958007       0.605922              0.887838                  10085.4       \n",
      "\u001b[J73          0.102932    0.966013       0.581382              0.892615                  10226.2       \n",
      "\u001b[J74          0.096179    0.96905        0.610044              0.88754                   10366.9       \n",
      "\u001b[J75          0.09071     0.96981        0.618749              0.887142                  10507.7       \n",
      "\u001b[J76          0.0929926   0.970351       0.631969              0.888834                  10648.4       \n",
      "\u001b[J77          0.0982113   0.96913        0.710645              0.89371                   10789.2       \n",
      "\u001b[J78          0.0957075   0.96971        0.633343              0.887341                  10929.9       \n",
      "\u001b[J79          0.100982    0.96929        0.591202              0.889729                  11070.6       \n",
      "\u001b[J80          0.0951255   0.971011       0.641238              0.891521                  11211.3       \n",
      "\u001b[J81          0.100187    0.96903        0.539707              0.895701                  11352.2       \n",
      "\u001b[J82          0.0921093   0.971131       0.727151              0.88953                   11493         \n",
      "\u001b[J83          0.0876625   0.972791       0.764655              0.893213                  11633.7       \n",
      "\u001b[J84          0.0945029   0.97019        0.663165              0.894307                  11774.5       \n",
      "\u001b[J85          0.0957749   0.969389       0.693099              0.886843                  11915.6       \n",
      "\u001b[J86          0.0900513   0.972471       0.583081              0.889928                  12056.4       \n",
      "\u001b[J87          0.0901928   0.971791       0.665705              0.891421                  12197.3       \n",
      "\u001b[J88          0.0915212   0.971131       0.670395              0.893412                  12338.1       \n",
      "\u001b[J89          0.0906646   0.971967       0.774577              0.88545                   12479.1       \n",
      "\u001b[J90          0.0903573   0.971891       0.639792              0.890426                  12619.9       \n",
      "\u001b[J91          0.086862    0.972711       0.607003              0.885848                  12760.7       \n",
      "\u001b[J92          0.0894998   0.972511       0.657595              0.888933                  12901.6       \n",
      "\u001b[J93          0.0855892   0.973426       0.738468              0.888336                  13042.6       \n",
      "\u001b[J94          0.086059    0.972831       0.746739              0.890824                  13183.4       \n",
      "\u001b[J95          0.0907545   0.972231       0.561937              0.88953                   13324.3       \n",
      "\u001b[J96          0.0789937   0.975252       0.715754              0.888137                  13465.1       \n",
      "\u001b[J97          0.0809737   0.974764       0.702233              0.892217                  13606.1       \n",
      "\u001b[J98          0.0793348   0.975252       0.745398              0.894705                  13747         \n",
      "\u001b[J99          0.0786371   0.974872       0.611804              0.89381                   13887.8       \n",
      "\u001b[J100         0.0801273   0.975292       0.681719              0.892317                  14028.6       \n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.datasets import cifar\n",
    "from chainer import iterators\n",
    "from chainer import optimizers\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "class ConvBlock(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, n_ch, pool_drop=False):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(ConvBlock, self).__init__(\n",
    "            conv=L.Convolution2D(None, n_ch, 3, 1, 1,\n",
    "                                 nobias=True, initialW=w),\n",
    "            bn=L.BatchNormalization(n_ch)\n",
    "        )\n",
    "        \n",
    "        self.train = True\n",
    "        self.pool_drop = pool_drop\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn(self.conv(x)))\n",
    "        if self.pool_drop:\n",
    "            h = F.max_pooling_2d(h, 2, 2)\n",
    "            chainer.using_config('train', self.train)\n",
    "            h = F.dropout(h, ratio=0.25)\n",
    "        return h\n",
    "    \n",
    "class LinearBlock(chainer.Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(LinearBlock, self).__init__(\n",
    "            fc=L.Linear(None, 1024, initialW=w))\n",
    "        self.train = True\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        chainer.using_config('train', self.train)\n",
    "        return F.dropout(F.relu(self.fc(x)), ratio=0.5)\n",
    "    \n",
    "class DeepCNN(chainer.ChainList):\n",
    "\n",
    "    def __init__(self, n_output):\n",
    "        super(DeepCNN, self).__init__(\n",
    "            ConvBlock(64),\n",
    "            ConvBlock(64, True),\n",
    "            ConvBlock(128),\n",
    "            ConvBlock(128, True),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256, True),\n",
    "            LinearBlock(),\n",
    "            LinearBlock(),\n",
    "            L.Linear(None, n_output)\n",
    "        )\n",
    "        self._train = True\n",
    "            \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train\n",
    "            \n",
    "    @train.setter\n",
    "    def train(self, val):\n",
    "        self._train = val\n",
    "        for c in self.children():\n",
    "            c.train = val\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for f in self.children():\n",
    "            x = f(x)\n",
    "        return x\n",
    "\n",
    "def train(model_object, batchsize=64, gpu_id=gpu_id, max_epoch=20):\n",
    "\n",
    "    # 1. Dataset\n",
    "    train, test = CIFAR10Augmented(), CIFAR10Augmented(False)\n",
    "\n",
    "    # 2. Iterator\n",
    "    train_iter = iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = iterators.SerialIterator(test, batchsize, False, False)\n",
    "\n",
    "    # 3. Model\n",
    "    model = L.Classifier(model_object)\n",
    "    if gpu_id >= 0:\n",
    "            model.to_gpu(gpu_id)\n",
    "\n",
    "    # 4. Optimizer\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    # 5. Updater\n",
    "    updater = training.StandardUpdater(train_iter, optimizer, device=gpu_id)\n",
    "\n",
    "    # 6. Trainer\n",
    "    trainer = training.Trainer(updater, (max_epoch, 'epoch'), out='{}_cifar10augmented_result'.format(model_object.__class__.__name__))\n",
    "\n",
    "    # 7. Evaluator\n",
    "\n",
    "    class TestModeEvaluator(extensions.Evaluator):\n",
    "\n",
    "        def evaluate(self):\n",
    "            model = self.get_target('main')\n",
    "            model.train = False\n",
    "            ret = super(TestModeEvaluator, self).evaluate()\n",
    "            model.train = True\n",
    "            return ret\n",
    "\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(TestModeEvaluator(test_iter, model, device=gpu_id))\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'validation/main/loss', 'validation/main/accuracy', 'elapsed_time']))\n",
    "    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "    trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "    trainer.run()\n",
    "    del trainer\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = train(DeepCNN(10), max_epoch=100, gpu_id=gpu_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
