{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to write a training loop in Chainer\n",
    "\n",
    "In this notebook session we will learn how to train a deep neural network to classify hand-written digits using the popular MNIST dataset. This dataset contains 50000 training examples and 10000 test examples. Each example contains a 28x28 greyscale image and a corresponding class label for the digit. Since the digits 0-9 are used, there are 10 class labels. \n",
    "\n",
    "Chainer provides a feature called `Trainer` that can be used to simplify the training process. However, we think it is good for first-time users to understand how the training process works before using the `Trainer` feature. Even advanced users might sometimes want to write their own training loop and so we will explain how to do so here. The complete training process consists of the following steps:\n",
    "\n",
    "1. Prepare a datasets that contain the train/validation/test examples.\n",
    "2. Optionally set of iterators for the datasets.\n",
    "3. Write a training loop that performs the following operations in each iteration:\n",
    "     1. Retreive batches of examples from the training dataset.\n",
    "     2. Feed the batches into the model.\n",
    "     3. Run the forward pass on the model to compute the loss.\n",
    "     4. Run the backward pass on the model to compute the gradients.\n",
    "     5. Run the optimizer on the model to update the parameters.\n",
    "     6. (Optional): Ocassionally check the performance on a validation/test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the dataset\n",
    "\n",
    "Chainer contains some built-in functions that can be used to download and return Chainer-formated versions of popular datasets used by the ML and deep learning communities. In this example, we will use the buil-in function that retreives the MNIST dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADgZJREFUeJzt3X+IXfWZx/HPs7H5wzQaZ0vHkMZNRyQSg53CGBcJa8Wd\n+oNIHBXpgJDFkOkfSbGwhJX0jypLJKwmS4NSZkpjk6WbZkElMZTGmqjp4hIcY/w1bqorKZ1hTCpx\nzA9/ZCfz7B/3THeqc793cu+599yZ5/2CYe49zzn3PBzyyfl552vuLgDx/FXRDQAoBuEHgiL8QFCE\nHwiK8ANBEX4gKMIPBEX4gaAIPxDURY1cmZnxOCFQZ+5uU5mvpj2/md1qZkfN7D0ze7CWzwLQWFbt\ns/1mNkvS7yV1ShqU9IqkbncfSCzDnh+os0bs+ZdJes/d33f3c5J+JWllDZ8HoIFqCf8CSX+c8H4w\nm/YXzKzHzPrNrL+GdQHIWd0v+Ll7n6Q+icN+oJnUsucfkrRwwvtvZNMATAO1hP8VSVeZ2TfNbLak\n70nak09bAOqt6sN+dx81s3WS9kmaJWmbu7+dW2cA6qrqW31VrYxzfqDuGvKQD4Dpi/ADQRF+ICjC\nDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAo\nwg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgqh6iW5LM7Jik05LOSxp19448mkJ+\nZs2alaxfeumldV3/unXrytYuvvji5LKLFy9O1teuXZusP/bYY2Vr3d3dyWU/++yzZH3Tpk3J+sMP\nP5ysN4Oawp+5yd0/zOFzADQQh/1AULWG3yU9b2avmllPHg0BaIxaD/uXu/uQmX1d0m/N7L/d/eDE\nGbL/FPiPAWgyNe353X0o+31C0jOSlk0yT5+7d3AxEGguVYffzOaY2dzx15K+K+mtvBoDUF+1HPa3\nSnrGzMY/59/d/Te5dAWg7qoOv7u/L+lbOfYyY11xxRXJ+uzZs5P1G264IVlfvnx52dq8efOSy959\n993JepEGBweT9a1btybrXV1dZWunT59OLvv6668n6y+99FKyPh1wqw8IivADQRF+ICjCDwRF+IGg\nCD8QlLl741Zm1riVNVB7e3uyfuDAgWS93l+rbVZjY2PJ+v3335+snzlzpup1Dw8PJ+sfffRRsn70\n6NGq111v7m5TmY89PxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExX3+HLS0tCTrhw4dStbb2trybCdX\nlXofGRlJ1m+66aaytXPnziWXjfr8Q624zw8gifADQRF+ICjCDwRF+IGgCD8QFOEHgspjlN7wTp48\nmayvX78+WV+xYkWy/tprryXrlf6EdcqRI0eS9c7OzmT97Nmzyfo111xTtvbAAw8kl0V9secHgiL8\nQFCEHwiK8ANBEX4gKMIPBEX4gaAqfp/fzLZJWiHphLsvzaa1SNolaZGkY5Ludff0HzrXzP0+f60u\nueSSZL3ScNK9vb1la6tXr04ue9999yXrO3fuTNbRfPL8Pv8vJN36hWkPStrv7ldJ2p+9BzCNVAy/\nux+U9MVH2FZK2p693i7pzpz7AlBn1Z7zt7r7+HhHH0hqzakfAA1S87P97u6pc3kz65HUU+t6AOSr\n2j3/cTObL0nZ7xPlZnT3PnfvcPeOKtcFoA6qDf8eSauy16sk7c6nHQCNUjH8ZrZT0n9JWmxmg2a2\nWtImSZ1m9q6kv8/eA5hGKp7zu3t3mdLNOfcS1qlTp2pa/uOPP6562TVr1iTru3btStbHxsaqXjeK\nxRN+QFCEHwiK8ANBEX4gKMIPBEX4gaAYonsGmDNnTtnas88+m1z2xhtvTNZvu+22ZP25555L1tF4\nDNENIInwA0ERfiAowg8ERfiBoAg/EBThB4LiPv8Md+WVVybrhw8fTtZHRkaS9RdeeCFZ7+/vL1t7\n4oknkss28t/mTMJ9fgBJhB8IivADQRF+ICjCDwRF+IGgCD8QFPf5g+vq6krWn3zyyWR97ty5Va97\nw4YNyfqOHTuS9eHh4WQ9Ku7zA0gi/EBQhB8IivADQRF+ICjCDwRF+IGgKt7nN7NtklZIOuHuS7Np\nD0laI+lP2Wwb3P3XFVfGff5pZ+nSpcn6li1bkvWbb65+JPfe3t5kfePGjcn60NBQ1euezvK8z/8L\nSbdOMv1f3b09+6kYfADNpWL43f2gpJMN6AVAA9Vyzv8DM3vDzLaZ2WW5dQSgIaoN/08ltUlqlzQs\naXO5Gc2sx8z6zaz8H3MD0HBVhd/dj7v7eXcfk/QzScsS8/a5e4e7d1TbJID8VRV+M5s/4W2XpLfy\naQdAo1xUaQYz2ynpO5K+ZmaDkn4s6Ttm1i7JJR2T9P069gigDvg+P2oyb968ZP2OO+4oW6v0twLM\n0rerDxw4kKx3dnYm6zMV3+cHkET4gaAIPxAU4QeCIvxAUIQfCIpbfSjM559/nqxfdFH6MZTR0dFk\n/ZZbbilbe/HFF5PLTmfc6gOQRPiBoAg/EBThB4Ii/EBQhB8IivADQVX8Pj9iu/baa5P1e+65J1m/\n7rrrytYq3cevZGBgIFk/ePBgTZ8/07HnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGguM8/wy1evDhZ\nX7duXbJ+1113JeuXX375Bfc0VefPn0/Wh4eHk/WxsbE825lx2PMDQRF+ICjCDwRF+IGgCD8QFOEH\ngiL8QFAV7/Ob2UJJOyS1SnJJfe7+EzNrkbRL0iJJxyTd6+4f1a/VuCrdS+/u7i5bq3Qff9GiRdW0\nlIv+/v5kfePGjcn6nj178mwnnKns+Ucl/aO7L5H0t5LWmtkSSQ9K2u/uV0nan70HME1UDL+7D7v7\n4ez1aUnvSFogaaWk7dls2yXdWa8mAeTvgs75zWyRpG9LOiSp1d3Hn6/8QKXTAgDTxJSf7Tezr0p6\nStIP3f2U2f8PB+buXm4cPjPrkdRTa6MA8jWlPb+ZfUWl4P/S3Z/OJh83s/lZfb6kE5Mt6+597t7h\n7h15NAwgHxXDb6Vd/M8lvePuWyaU9khalb1eJWl3/u0BqJeKQ3Sb2XJJv5P0pqTx70huUOm8/z8k\nXSHpDyrd6jtZ4bNCDtHd2pq+HLJkyZJk/fHHH0/Wr7766gvuKS+HDh1K1h999NGytd270/sLvpJb\nnakO0V3xnN/d/1NSuQ+7+UKaAtA8eMIPCIrwA0ERfiAowg8ERfiBoAg/EBR/unuKWlpaytZ6e3uT\ny7a3tyfrbW1tVfWUh5dffjlZ37x5c7K+b9++ZP3TTz+94J7QGOz5gaAIPxAU4QeCIvxAUIQfCIrw\nA0ERfiCoMPf5r7/++mR9/fr1yfqyZcvK1hYsWFBVT3n55JNPyta2bt2aXPaRRx5J1s+ePVtVT2h+\n7PmBoAg/EBThB4Ii/EBQhB8IivADQRF+IKgw9/m7urpqqtdiYGAgWd+7d2+yPjo6mqynvnM/MjKS\nXBZxsecHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaDM3dMzmC2UtENSqySX1OfuPzGzhyStkfSnbNYN\n7v7rCp+VXhmAmrm7TWW+qYR/vqT57n7YzOZKelXSnZLulXTG3R+balOEH6i/qYa/4hN+7j4saTh7\nfdrM3pFU7J+uAVCzCzrnN7NFkr4t6VA26Qdm9oaZbTOzy8os02Nm/WbWX1OnAHJV8bD/zzOafVXS\nS5I2uvvTZtYq6UOVrgP8s0qnBvdX+AwO+4E6y+2cX5LM7CuS9kra5+5bJqkvkrTX3ZdW+BzCD9TZ\nVMNf8bDfzEzSzyW9MzH42YXAcV2S3rrQJgEUZypX+5dL+p2kNyWNZZM3SOqW1K7SYf8xSd/PLg6m\nPos9P1BnuR7254XwA/WX22E/gJmJ8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAo\nwg8ERfiBoAg/EFSjh+j+UNIfJrz/WjatGTVrb83al0Rv1cqzt7+Z6owN/T7/l1Zu1u/uHYU1kNCs\nvTVrXxK9Vauo3jjsB4Ii/EBQRYe/r+D1pzRrb83al0Rv1Sqkt0LP+QEUp+g9P4CCFBJ+M7vVzI6a\n2Xtm9mARPZRjZsfM7E0zO1L0EGPZMGgnzOytCdNazOy3ZvZu9nvSYdIK6u0hMxvKtt0RM7u9oN4W\nmtkLZjZgZm+b2QPZ9EK3XaKvQrZbww/7zWyWpN9L6pQ0KOkVSd3uPtDQRsows2OSOty98HvCZvZ3\nks5I2jE+GpKZ/Yukk+6+KfuP8zJ3/6cm6e0hXeDIzXXqrdzI0v+gArddniNe56GIPf8ySe+5+/vu\nfk7SryStLKCPpufuByWd/MLklZK2Z6+3q/SPp+HK9NYU3H3Y3Q9nr09LGh9ZutBtl+irEEWEf4Gk\nP054P6jmGvLbJT1vZq+aWU/RzUyidcLISB9Iai2ymUlUHLm5kb4wsnTTbLtqRrzOGxf8vmy5u7dL\nuk3S2uzwtil56ZytmW7X/FRSm0rDuA1L2lxkM9nI0k9J+qG7n5pYK3LbTdJXIdutiPAPSVo44f03\nsmlNwd2Hst8nJD2j0mlKMzk+Pkhq9vtEwf38mbsfd/fz7j4m6WcqcNtlI0s/JemX7v50NrnwbTdZ\nX0VttyLC/4qkq8zsm2Y2W9L3JO0poI8vMbM52YUYmdkcSd9V840+vEfSquz1Kkm7C+zlLzTLyM3l\nRpZWwduu6Ua8dveG/0i6XaUr/v8j6UdF9FCmrzZJr2c/bxfdm6SdKh0G/q9K10ZWS/prSfslvSvp\neUktTdTbv6k0mvMbKgVtfkG9LVfpkP4NSUeyn9uL3naJvgrZbjzhBwTFBT8gKMIPBEX4gaAIPxAU\n4QeCIvxAUIQfCIrwA0H9H4BpmwJXvvG+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5c7e7449e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    }
   ],
   "source": [
    "from chainer.datasets import mnist\n",
    "\n",
    "# Download the MNIST data if you haven't downloaded it yet\n",
    "train, test = mnist.get_mnist(withlabel=True, ndim=1)\n",
    "\n",
    "# set matplotlib so that we can see our drawing inside this notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display an example from the MNIST dataset.\n",
    "# `x` contains the input image array and `t` contains that target class\n",
    "# label as an integer.\n",
    "x, t = train[0]\n",
    "plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "print('label:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the dataset iterators\n",
    "\n",
    "Although this is an optional step, it can often be convinient to use iterators that operate on a dataset and return a certain number of examples (often called a \"mini-batch\") at a time. The number of examples that is returned at a time is called the \"batch size\" or \"mini-batch size.\" Chainer already has an `Iterator` class and some subclasses that can be used for this purpose and it is straightforward for users to write their own as well.\n",
    "\n",
    "We will use the `SerialIterator` subclass of `Iterator` in this example. The `SerialIterator` can either return the examples in the same order that they appear in the dataset (that is, in sequetial order) or can shuffle the examples so that they are returned in a random order.\n",
    "\n",
    "An Iterator can return a new minibutch by calling its 'next()' method. An Iterator also has properties to manage the training such as 'epoch': how many times we have gone through the entire dataset, 'is_new_epoch': whether the current iteration is the first iteration of a new epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from chainer import iterators\n",
    "\n",
    "# Choose the minibatch size.\n",
    "batchsize = 128\n",
    "\n",
    "train_iter = iterators.SerialIterator(train, batchsize)\n",
    "test_iter = iterators.SerialIterator(test, batchsize,\n",
    "                                     repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details about SerialIterator\n",
    "\n",
    "- `SerialIterator` is a built-in subclass of `Iterator` that can be used to retreive a dataset in either sequential or shuffled order.\n",
    "- The `Iterator` initializer takes two arguments: the dataset object and a batch size. \n",
    "- When data need to be used repeatedly for training, set the 'repeat' argument to 'True' (the default). When data is needed only once and no longer necessary for retriving the data anymore, set 'repeat' to 'False'.\n",
    "- When you want to shuffle the training dataset for every epoch, set the 'shuffle' argument 'True'.\n",
    "\n",
    "In the example above, we set 'batchsize = 128', 'train_iter'is the Iterator for the training dataset, and 'test_iter' is the Iterator for test dataset. These iterators will therefore return 128 image examples as a bundle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the model\n",
    "\n",
    "Now let's define a neural network that we will train to classify the MNIST images. For simplicity, we will use a fully-connected network with three layers. We will set each hidden layer to have 100 units and set the output layer to have 10 units, corresponding to the 10 class labels for the MNIST digits 0-9. \n",
    "\n",
    "We first briefly explain `Link`, `Function`, `Chain`, and `Variable` which are the basic components used for defining and running a model in Chainer. \n",
    "\n",
    "### `Link` and `Function`\n",
    "\n",
    "In Chainer, each layer of a neural network is decomposed into one of two broad types of functions (actually, they are function objects): 'Link' and 'Function'.\n",
    "\n",
    "- ** `Function` is a function without learnable paremeters**\n",
    "- ** `Link` is a function that conatins (learnable) parameters** We can think of `Link` as wrapping a `Function` to give it parameters. That is, `Link` will contain the parameters and when it is called, it will also call a corresponding `Function`.\n",
    "\n",
    "We then describe a model by implementing code the performs the \"forward pass\" computations. This code will call various links and chains (recall that `Link` and `Chain` are callable objects). Chainer will take care of the \"backward pass\" automatically and so we do not need to worry about that unless we want to write some custom funcions.\n",
    "\n",
    "- For examples of links, see the 'chainer.links' module.\n",
    "- For examples of functions, see the 'chainer.functions' module.\n",
    "- For example, see the `Linear` link, which wraps the `linear` function to give it weight and bias parameters.\n",
    "\n",
    "Before we can start using them, we first need to import the modules as shown below.\n",
    "\n",
    "    ```\n",
    "    import chainer.links as L\n",
    "    import chainer.functions as F\n",
    "    ```\n",
    "The Chainer convention is to use `L` for links and `F` for functions, like 'L.Convolution2D(...)' or 'F.relu(...)'.\n",
    "\n",
    "### `Chain`\n",
    "\n",
    "- `Chain` is a class that can hold multiple links and/or functions. It is a subclass of `Link` and so it is also a `Link`. \n",
    "- This means that a chain can contain parameters, which are the parameters of any links that it deeply contains.\n",
    "- In this way, `Chain` allows us to construct models with a potentially deep hierarchy of functions and links.\n",
    "- It is often convinient to use a single chain that contains all of the layers (other chains, links, and functions) of the model. This is because we will need to optimize the model's parameters during training and if all of the parameters are contained by a single chain, it turns out to be straightfoward to pass these parameters into an optimizer (which we describe in more detail below).\n",
    "\n",
    "### `Variable`\n",
    "In Chainer, both the activations (that is, the inputs and outputs of function and links) and the model parmaeters are instances of the `Variable` class. A `Variable` holds two arrays: a `data` array that contains the values that are read/written during the forward pass (or the parameter values), and a `grad` array that contains the corresponding gradients that will are computed during the backward pass.\n",
    "\n",
    "A `Variable` can potentially contain two types of arrays as well, depending whether the array resides in CPU or GPU memory. By default, the CPU is used and these will be Numpy arrays. However, it is possible to move or create these arrays on the GPU as well, in which case they will be CuPy arrays. Fortunately, CuPy uses an API that is nearly identical to Numpy. This is convinient because in addition to making it easier for users to learn (there is almost nothing to learn if you are already familiar with Numpy), it often allows us to reuse the same code for both Numpy and CuPy arrays.\n",
    "\n",
    "\n",
    "### Create our model as a subclass of 'Chain' \n",
    "\n",
    "We can create our model be writing a new subclass of `Chain`. The two main steps are:\n",
    "\n",
    "1. Any links (possiblye also including other chains) that we wish to call during the forward computation of our chain must first be supplied to the chain's `__init__` method. After the `__init__` method has been called, these links will then be accessable as attributes of our chain object. This means that we also need to provide the attribute name that we want to use for each link that is supplied. We do this by providing the attribute name and corresponding link object as keyword arguments to `__init__`, as we will do in the `MLP` chain below.\n",
    "2. We need to define a `__call__` method that allows our chain to be called like a function. This method takes one or more `Variable` objects as input (that is, the input activations) and returns one or more `Variable` objects. This method executes the forward pass of the model by calling any of the links that we supplied to `__init__` earlier as well as any functions.\n",
    "\n",
    "Note that only the links need to be supplied to `__init__`. This is because they contain parameters. Since functions do not contain any parameters, they can be called in `__call__` without having to supply them to the chain beforehand. For example, we can use a function such as `F.relu` by simply calling it in `__call__` but a link such as `L.Linear` would need to first be supplied to the chain's `__init__` in order to call it in `__call__`.\n",
    "\n",
    "If we decide that we want to call a link in a chain after `__init__` has already been called, we can use the `add_link` method of `Chain` to add a new link at any time.\n",
    "\n",
    "\n",
    "\n",
    "### How to run a model on GPU\n",
    "\n",
    "- The `Link` and `Chain` classes have a `to_gpu` method that takes a GPU id argument specifying which GPU to use. This method sends all of the model paremeters to GPU memory.\n",
    "- By default, the CPU is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x7f5c3528aeb8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "class MLP(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_mid_units=100, n_out=10):\n",
    "        # register layers with parameters by super initializer\n",
    "        super(MLP, self).__init__(\n",
    "            l1=L.Linear(None, n_mid_units),\n",
    "            l2=L.Linear(None, n_mid_units),\n",
    "            l3=L.Linear(None, n_out),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # describe the forward pass, given x (input data)\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)\n",
    "\n",
    "gpu_id = 0\n",
    "\n",
    "model = MLP()\n",
    "model.to_gpu(gpu_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "The `L.Linear` class is a link that represents a fully connected layer. When 'None' is passed as the first argument, this allows the number of necessary input units (`n_input`) and also the size of the weight and bias parameters to be automatically determined and computed at runtime during the first forward pass. We call this feature `parameter shape placeholder`. This can be a very helpful feature when defining deep neural network models, since it would often be tedious to manually determine these input sizes. \n",
    "\n",
    "As mentioned previously, a `Link` can contain multiple parameter arrays. For example, the `L.Linear` link conatins two parameter arrays: the weigts `W` and bias `b`. Recall that for a given link or chain, such as the `MLP` chain above, the links it contains can be accessed as attributes (or properties). The paramaters of a link can also be accessed as attributes. For example, following code shows how to access the bias parameter of layer l1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the bias of the first layer, l1, in the model、 (100,)\n",
      "The values of the bias of the first layer in the model after initialization、 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the bias of the first layer, l1, in the model、', model.l1.b.shape)\n",
    "print('The values of the bias of the first layer in the model after initialization、', model.l1.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select an optimization algorithm\n",
    "\n",
    "Chainer provides a wide variety of optimization algorithms that can be used to optimize the model parameters during training. They are located in the `chainer.optimizers` module. \n",
    "\n",
    "Here, we are going to use the basic stochastic gradient descent (SGD) method, which is implemented by `optimizers.SGD`. The model (recall that it is a`Chain` object) we created is passed to the optimizer object by providing the model as an argument to the optimizer's 'setup' method. In this way, Optimizer can automatically find the model paremeters to be optimized. \n",
    "\n",
    "You can easily try out other optimizers as well. Please test and observe the results of various optimizers. For example, you could try to change 'SGD\" of 'chainer.optimizers.SGD' to 'MomentumSGD', 'RMSprop', 'Adam', etc and run your training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from chainer import optimizers\n",
    "\n",
    "# Choose an optimizer algorithm\n",
    "optimizer = optimizers.SGD(lr=0.01)\n",
    "# Give the optimizer a reference to the model so that it\n",
    "# can locate the model's parameters.\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "Observe that above, we set`lr` to 0.01 in the SGD constructor. This value is known as a the \"learning rate\", one of the most important ** hyper paremeters** that need be adjusted in order to obtain the best performance. The various optimizers may each have differnt hyper-parameters and so be sure to check the documentation for the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write the training loop\n",
    "\n",
    "We now show how to write the training loop. Since we are working on a digit classification problem, we will use `softmax_cross_entropy` as the loss function for the optimizer to minimize. For other types of problems, such as regression models, other loss functions might be more appropriate. See the Chainer documentation for detailed information on the various loss functions that are available.\n",
    "\n",
    "Our training loop will be structured as follows. We will first get a mini-batch of examples from the training dataset. We will then feed the batch into our model by calling or model (a `Chain` object) like a function. This will execute the forward-pass code that we wrote for the chain's `__call__` method that we wrote above. This will cause the model to output class label predictions that we supply to the loss function along with the true (that is, target) values. The loss function will output the loss as a `Variable` object. We then clear any previous gradients and perform the backward pass by calling the `backward` method on the loss variable which computes the parameter gradients. We need to clear the gradients first because the `backward` method accumulates gradients instead of overwriting the previous values. Since the optimizer already was given a reference to the model, it already has access to the parameters and the newly-computed gradients and so we can now call the `update` method of the optimizer which will update the model parameters.\n",
    "\n",
    "At this point you might be wondering how calling `backward` on the loss variable could possibly compute the gradients for all of the model parameters. This works as follows. First recall that all activation and parameter arrays in the model are instances of `Variable`. During the forward pass, as each function is called on its inputs, we save references in each output variable that refer to the function that created it and its input variables. In this way, by the time the final loss variable is computed, it actually contains backward references that lead all the way back to the input variables of the model. That is, the loss variable contains a representation of the entire computational graph of the model, which is recomputed each time the forward pass is performed. By following these backward references from the loss variable, each function as a `backward` method that gets called to compute any parameter gradients. Thus, by the time the end of the backward graph is reached (at the input variables of the model), all parameter gradients have been computed.  \n",
    "\n",
    "Thus, there are four steps in single training loop iteration as shown below.\n",
    "\n",
    "1. Obtain and pass a mini-batch of example images into the model and obtain the output digit predictions `prediction_train`. \n",
    "2. Compute the loss function, giving it the predicted labels from the output of our model and also the true \"target\" label values.\n",
    "3. Clear any previous gradients and call the backward method of 'Variable' to compute the parameter gradients for the model.  \n",
    "4. Call the 'update' method of Optimizer, which performs one optimization step and updates all of the model parameters.\n",
    "\n",
    "In addition to the above steps, it is good to occasionally check the performance of our model on a validation and/or test set. This allows us to observe how well it can generalize to new data and also check whether it is overfitting. The code below checks the performance on the test set at the end of each epoch. The code has the same structure as the training code except that no backpropagation is performed and we also commpute the accuracy on the test set using the `F.accuracy` function.\n",
    "\n",
    "We can write the training loop code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:01 train_loss:0.8072 val_loss:0.7592 val_accuracy:0.8289\n",
      "epoch:02 train_loss:0.5021 val_loss:0.4467 val_accuracy:0.8841\n",
      "epoch:03 train_loss:0.3539 val_loss:0.3673 val_accuracy:0.9007\n",
      "epoch:04 train_loss:0.2524 val_loss:0.3307 val_accuracy:0.9067\n",
      "epoch:05 train_loss:0.4232 val_loss:0.3076 val_accuracy:0.9136\n",
      "epoch:06 train_loss:0.3033 val_loss:0.2910 val_accuracy:0.9167\n",
      "epoch:07 train_loss:0.2004 val_loss:0.2773 val_accuracy:0.9222\n",
      "epoch:08 train_loss:0.2885 val_loss:0.2679 val_accuracy:0.9239\n",
      "epoch:09 train_loss:0.2818 val_loss:0.2579 val_accuracy:0.9266\n",
      "epoch:10 train_loss:0.2403 val_loss:0.2484 val_accuracy:0.9307\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chainer.dataset import concat_examples\n",
    "from chainer.cuda import to_cpu\n",
    "\n",
    "max_epoch = 10\n",
    "\n",
    "while train_iter.epoch < max_epoch:\n",
    "    \n",
    "    # ---------- The first iteration of Trainig loop ----------\n",
    "    train_batch = train_iter.next()\n",
    "    image_train, target_train = concat_examples(train_batch, gpu_id)\n",
    "    \n",
    "    # calculate the prediction of the model\n",
    "    prediction_train = model(image_train)\n",
    "\n",
    "    # calculation of loss function, softmax_cross_entropy\n",
    "    loss = F.softmax_cross_entropy(prediction_train, target_train)\n",
    "\n",
    "    # calculate the gradients in the model\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    # update the paremters of the model\n",
    "    optimizer.update()\n",
    "    # --------------- until here One loop ----------------\n",
    "    \n",
    "    # Check if the generalization of the model is improving \n",
    "    # by measuring the accuracy of prediction after every epoch\n",
    "\n",
    "    if train_iter.is_new_epoch:  # after finishing the first epoch\n",
    "\n",
    "        # display the result of the loss function\n",
    "        print('epoch:{:02d} train_loss:{:.04f} '.format(\n",
    "            train_iter.epoch, float(to_cpu(loss.data))), end='')\n",
    "\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        while True:\n",
    "            test_batch = test_iter.next()\n",
    "            image_test, target_test = concat_examples(test_batch, gpu_id)\n",
    "\n",
    "            # forward the test data\n",
    "            prediction_test = model(image_test)\n",
    "\n",
    "            # calculate the loss function\n",
    "            loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
    "            test_losses.append(to_cpu(loss_test.data))\n",
    "\n",
    "            # calculate the accuracy\n",
    "            accuracy = F.accuracy(prediction_test, target_test)\n",
    "            accuracy.to_cpu()\n",
    "            test_accuracies.append(accuracy.data)\n",
    "            \n",
    "            if test_iter.is_new_epoch:\n",
    "                test_iter.epoch = 0\n",
    "                test_iter.current_position = 0\n",
    "                test_iter.is_new_epoch = False\n",
    "                test_iter._pushed_position = None\n",
    "                break\n",
    "\n",
    "        print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
    "            np.mean(test_losses), np.mean(test_accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the trained model\n",
    "\n",
    "Chainer provides two types of serializers that can be used to save and restore model state. One supports the HDF5 format and the other supports the Numpy NPZ format. For this example, we are going to use the NPZ format to save our model since it is easy to use with Numpy without requiring an additional dependencies or libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 brian brian 334037  4月 26 15:01 my_mnist.model\r\n"
     ]
    }
   ],
   "source": [
    "from chainer import serializers\n",
    "\n",
    "serializers.save_npz('my_mnist.model', model)\n",
    "\n",
    "# check if the model is saved.\n",
    "%ls -la my_mnist.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Perform classification by restoring a previously trained model \n",
    "\n",
    "We will now use our previously trained and saved MNIST model to classify a new image. In order to load a previously-trained model, we need to perform the following two steps:\n",
    "1. We must use the same model definition code the was used to create the previously-trained model. For our example, this is the `MLP` chain that we created earlier.\n",
    "2. We then overwrite any parameters in the newly-created model with the values that were saved earlier using the serializer. The `serializers.load_npz` function can be used to do this.\n",
    "\n",
    "Now that the model has been restored, it can be used to predict image labels on new input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x7f5c78b7ccc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the infrence (evaluation) model as the preivious model\n",
    "infer_model = MLP()\n",
    "\n",
    "# Load the saved paremeters into the parametes of the new inference model to overwrite \n",
    "serializers.load_npz('my_mnist.model', infer_model)\n",
    "\n",
    "# Send the model to utilize GPU by to_GPU\n",
    "infer_model.to_gpu(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH0iK8ANJEX4gKcIPJEX4gaQIP5DUXwFGhz+pWT5yuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5c35314668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 7\n"
     ]
    }
   ],
   "source": [
    "# Get a test image and label\n",
    "x, t = test[0]\n",
    "plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "print('label:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,) -> (1, 784)\n",
      "predicted label: 7\n"
     ]
    }
   ],
   "source": [
    "from chainer.cuda import to_gpu\n",
    "\n",
    "# change the shape to minibutch. \n",
    "# In this example, the size of minibatch is 1. \n",
    "# Inference using any mini-batch size can be performed.\n",
    "\n",
    "print(x.shape, end=' -> ')\n",
    "x = x[None, ...]\n",
    "print(x.shape)\n",
    "\n",
    "# to calculate by GPU, send the data to GPU, too. \n",
    "x = to_gpu(x, 0)\n",
    "\n",
    "# forward calculation of the model by sending X\n",
    "y = infer_model(x)\n",
    "\n",
    "# The result is given as Variable, then we can take a look at the contents by the attribute, .data. \n",
    "y = y.data\n",
    "\n",
    "# send the gpu result to cpu\n",
    "y = to_cpu(y)\n",
    "\n",
    "# The most probable number by looking at the argmax\n",
    "pred_label = y.argmax(axis=1)\n",
    "\n",
    "print('predicted label:', pred_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
